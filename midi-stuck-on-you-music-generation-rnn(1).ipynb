{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "7a075ff0-be6d-455f-8d0a-b549af75ac99",
    "_uuid": "fe251a6a-2c47-44ba-9d94-9dc581386a02",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-04T08:32:12.837996Z",
     "iopub.status.busy": "2025-07-04T08:32:12.837547Z",
     "iopub.status.idle": "2025-07-04T08:32:41.004203Z",
     "shell.execute_reply": "2025-07-04T08:32:41.003559Z",
     "shell.execute_reply.started": "2025-07-04T08:32:12.837975Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretty_midi\n",
      "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting mido>=1.1.16 (from pretty_midi)\n",
      "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (25.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
      "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=76ab1671ed89497ff912c2ae86699176d72f2cedca94c238c4a5f98752be1797\n",
      "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
      "Successfully built pretty_midi\n",
      "Installing collected packages: mido, pretty_midi\n",
      "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (3.13.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (0.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0) (25.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras>=3.5.0) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.5.0) (4.13.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras>=3.5.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras>=3.5.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras>=3.5.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras>=3.5.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras>=3.5.0) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:32:27.990537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751617948.212756      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751617948.276355      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install pretty_midi numpy \n",
    "!pip install -q tensorflow\n",
    "!pip install 'keras>=3.5.0'\n",
    "\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Data \n",
    "Loading and printing basic information about the MIDI dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b5db61b7-7b12-4483-80be-4d3decfe185f",
    "_uuid": "a2b81218-182d-49f8-9bf5-16b53b11cf16",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-04T08:32:41.006096Z",
     "iopub.status.busy": "2025-07-04T08:32:41.005581Z",
     "iopub.status.idle": "2025-07-04T08:32:41.009789Z",
     "shell.execute_reply": "2025-07-04T08:32:41.009007Z",
     "shell.execute_reply.started": "2025-07-04T08:32:41.006071Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# configuration\n",
    "MIDI_FILE_PATH = \"/kaggle/input/lakh-midi-clean/\"\n",
    "SEQUENCE_LENGTH = 50 # number of notes in the imput sequence for the RNN\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e841f5d8-9c89-4a50-8402-f021c0ea49e8",
    "_uuid": "8788a692-1737-45f9-b681-96935089e23c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-04T08:32:41.010848Z",
     "iopub.status.busy": "2025-07-04T08:32:41.010648Z",
     "iopub.status.idle": "2025-07-04T08:32:41.183227Z",
     "shell.execute_reply": "2025-07-04T08:32:41.182409Z",
     "shell.execute_reply.started": "2025-07-04T08:32:41.010831Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load and print basic information about its contents\n",
    "def load_and_inspect_midi(midi_path):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "        print(f\"Successfully loaded MIDI file: {midi_path}\")\n",
    "        print(f\"MIDI file contains {len(midi_data.instruments)} instruments.\")\n",
    "\n",
    "        notes = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            # focus on notes\n",
    "            if not instrument.is_drum: # excludes drum tracks for melody generation \n",
    "                print(f\" Insrument: {instrument.name}, Program: {instrument.program} ({pretty_midi.program_to_instrument_name(instrument.program)})\")\n",
    "                for note in instrument.notes:\n",
    "                    # store pitch, start_time, end_time, velocity\n",
    "                    notes.append((note.pitch, note.start, note.end, note.velocity))\n",
    "\n",
    "        # sort notes by start time to maintain chronological order\n",
    "        notes.sort(key=lambda x: x[1])\n",
    "\n",
    "        print(f\"\\nTotal notes extracted (across all non_drum instruments): {len(notes)}\")\n",
    "        if notes:\n",
    "            print(f\"First 5 extracted notes: {notes[:5]}\")\n",
    "            print(f\"Last 5 extracted notes: {notes[-5:]}\")\n",
    "\n",
    "        else:\n",
    "            print(\"No notes found in the non_drum tracks\")\n",
    "\n",
    "        return notes, midi_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MIDI files: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_notes, _ = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "#     if extracted_notes:\n",
    "#         print(\"\\nProceeding to next steps for data processing and model building....\")\n",
    "#     else:\n",
    "#         print(\"Please ensure your MIDI_FILE_PATH is correct and the file is valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f5641ad-7bef-489d-80d1-e66147c32162",
    "_uuid": "20be112b-147b-4843-af1a-8f0c7b72a7e7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Step 2: Data Extraction and Simplification\n",
    "\n",
    "**Goal of this step:**\n",
    "\n",
    "1. Extract relevant features: From the (pitch, start, end, velocity) tuples, we'll primarily extract pitch and calculate a simplified duration.\n",
    "\n",
    "2. Handle polyphony (simplification): Since our goal is monophonic melodies, we need to decide how to handle cases where multiple notes overlap (i.e., chords). For simplicity, we'll likely pick only one note at a given time or just iterate through all notes, effectively flattening any chords into a sequence of individual notes played close together. The pretty_midi library already gives us individual notes, so if a chord is played, they'll just appear as separate notes with the same start time. For very simple generation, we might even just take the highest or lowest note of a chord, but for now, let's keep all individual notes.\n",
    "\n",
    "3. Quantize time: Musical durations are often quantized (e.g., quarter notes, eighth notes, sixteenth notes). We'll convert continuous start and end times into a more discrete duration representation. A simple way is to calculate duration = note.end - note.start. Another approach is to calculate the time between the onset of consecutive notes. For a simple RNN, predicting a sequence of (pitch, time_until_next_note) pairs is a common and effective strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1fe1b6a1-7595-4784-b27b-15705076ba40",
    "_uuid": "e21ea8a1-97f4-4545-8fc5-228a8e1853c6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:42:20.688646Z",
     "iopub.status.busy": "2025-07-03T16:42:20.688269Z",
     "iopub.status.idle": "2025-07-03T16:42:20.701168Z",
     "shell.execute_reply": "2025-07-03T16:42:20.700015Z",
     "shell.execute_reply.started": "2025-07-03T16:42:20.688618Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data extraction and simplfication\n",
    "def process_notes_for_rnn(notes_list, quantization_steps_per_beat=4):\n",
    "    \"\"\"\n",
    "    Converts a list of raw (pitch, start, end, velocity) tuples into\n",
    "    a simplified sequence of (pitch, time_delta) pairs suitable for an RNN.\n",
    "\n",
    "    Args:\n",
    "        notes_list: A list of tuples, each representing a note.\n",
    "                    (pitch, start_time, end_time, velocity)\n",
    "        quantization_steps_per_beat: How many sub beats per beat for time quantization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - sequence_of_events: A list of (pitch, time_delta) tuples.\n",
    "            - unique_pitches: All unique pitch values found.\n",
    "            - unique_time_deltas: All unique time_delta values found.\n",
    "            \n",
    "    \"\"\"\n",
    "    if not notes_list:\n",
    "        print(\"No notes to process.\")\n",
    "        return [], set(), set()\n",
    "\n",
    "    # Determine the tempo/beats_per_minute to convert seconds to beats\n",
    "    # For simplicity here, we assume a standard tempo or you might get it from PrettyMIDI's tempo map\n",
    "    # For now, let's just use raw seconds, or manually define a beat duration if not derived from MIDI\n",
    "    # If deriving from MIDI, you'd use midi_data.get_tempo_changes()\n",
    "    # For this,  we will just use relative time in seconds, and quantize it.\n",
    "\n",
    "    # calculate quantized_time deltas\n",
    "    sequence_of_events = []\n",
    "\n",
    "    # store all unique pitches and time deltas for vocabulary creation\n",
    "    unique_pitches = set()\n",
    "    unique_time_deltas = set()\n",
    "\n",
    "    # Convert notes into (pitch, time_delta) events\n",
    "    # We iterate through the sorted notes to calculate the time difference\n",
    "    # between the start of the current note and the start of the next note.\n",
    "    # This captures both note duration and rests implicitly.\n",
    "\n",
    "    current_time = 0.0 #initialize time tracking\n",
    "\n",
    "    # for the first note, its time_delta is from time 0.0 to its start time\n",
    "    if notes_list[0][1] > 0: # if the first note does not start from zero\n",
    "        initial_rest = notes_list[0][1]\n",
    "        quantized_initial_rest = round(initial_rest * quantization_steps_per_beat) / quantization_steps_per_beat\n",
    "        if quantized_initial_rest > 0:\n",
    "            sequence_of_events.append(('Rest', quantized_initial_rest))\n",
    "            unique_time_deltas.add(quantized_initial_rest)\n",
    "        current_time = notes_list[0][1]\n",
    "\n",
    "    for i in range(len(notes_list)):\n",
    "        pitch, start, end, velocity = notes_list[i]\n",
    "\n",
    "        # Handle rests and overlaps between consecutive notes\n",
    "        if i > 0:\n",
    "            prev_end = notes_list[i-1][2]\n",
    "            # if there's a gap(rest) between notes, add a 'Rest' event\n",
    "            if start > prev_end:\n",
    "                rest_duration = start - prev_end\n",
    "                quantized_rest_duration = round(rest_duration * quantization_steps_per_beat) / quantization_steps_per_beat\n",
    "                if quantized_rest_duration > 0:\n",
    "                    sequence_of_events.append(('Rest', quantized_rest_duration))\n",
    "                    unique_time_deltas.add(quantized_rest_duration)\n",
    "\n",
    "            # If notes overlap (polyphony), we are simplifying by just processing notes one by one\n",
    "            # The time_delta here captures the time from the *start* of the previous note to the *start* of the current note,\n",
    "            # or from the *end* of the previous note to the *start* of the current note (rest), plus the duration of the current note.\n",
    "            # A common simpler approach is just (pitch, duration) pairs, and *then* calculate time deltas for the RNN.\n",
    "            # Let's try (pitch, duration) first, it's simpler for monophonic.\n",
    "\n",
    "            # simple approach: (pitch, duration_of_note)\n",
    "            duration = end - start\n",
    "            quantized_duration = round(duration * quantization_steps_per_beat) / quantization_steps_per_beat\n",
    "\n",
    "            # Avoid 0 duration notes or avoid them\n",
    "            if quantized_duration <= 0:\n",
    "                quantized_duration = 1.0 / quantization_steps_per_beat # assign minimum duration\n",
    "\n",
    "            sequence_of_events.append((pitch, quantized_duration))\n",
    "            unique_pitches.add(pitch)\n",
    "            unique_time_deltas.add(quantized_duration)\n",
    "\n",
    "    print(f\"\\nProcessed {len(sequence_of_events)} events (pitch, duration) for RNN input.\")\n",
    "    print(f\"Unique pitches found: {sorted(list(unique_pitches))}\")\n",
    "    print(f\"Unique durations found: {sorted(list(unique_time_deltas))}\")\n",
    "    print(f\"First 10 processed events: {sequence_of_events[:10]}\")\n",
    "\n",
    "    return sequence_of_events, unique_pitches, unique_time_deltas\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_notes, midi_obj = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "    \n",
    "#     if extracted_notes:\n",
    "#         # For simplicity, let's assume a default tempo for quantization if not explicit\n",
    "#         # You can get tempo from midi_obj.estimate_tempos() if needed, but for simple files\n",
    "#         # a fixed quantization is fine. Let's use 4 steps per beat (quarter notes).\n",
    "        \n",
    "#         processed_events, unique_pitches_set, unique_time_deltas_set = process_notes_for_rnn(\n",
    "#             extracted_notes, quantization_steps_per_beat=4\n",
    "#         )\n",
    "#         processed_events = [f\"{pitch}_{duration}\" for pitch, duration in processed_events]\n",
    "        \n",
    "#         # Now you have `processed_events` which is a list of (pitch_duration) tuples.\n",
    "#         # `unique_pitches_set` and `unique_time_deltas_set` contain your vocabulary.\n",
    "        \n",
    "#         # For the next step (Step 3: Sequence Creation), you'll combine these into a single vocabulary\n",
    "#         # and create integer mappings.\n",
    "        \n",
    "#         print(\"\\nStep 2: Data Extraction & Simplification Complete.\")\n",
    "#         print(\"You now have a sequence of (pitch, duration) events and their unique values.\")\n",
    "#     else:\n",
    "#         print(\"MIDI file could not be processed. Please check the path and file validity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72c4773d-0e81-48a0-a634-020d8f6349a5",
    "_uuid": "f8fc76fb-0910-46b4-8580-fc8b6d418964",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Step 3: Sequence Creation\n",
    "\n",
    " In this step, we'll prepare the data in a format directly usable by TensorFlow/Keras for training an RNN. This involves:\n",
    "\n",
    "1. Creating a Unified Vocabulary: Combining all unique pitches and durations into a single set of unique \"events.\"\n",
    "\n",
    "2. Mapping to Integers: Assigning a unique integer ID to each unique event.\n",
    "\n",
    "3. Creating Input-Output Pairs: Sliding a window over your sequence of events to create the X (input) and y (target/output) pairs for your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01864bff-6cf0-47f4-b70e-ce9e68c79afb",
    "_uuid": "4251ec14-188f-449d-9f69-d3563c8ef3c7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:42:43.959240Z",
     "iopub.status.busy": "2025-07-03T16:42:43.958906Z",
     "iopub.status.idle": "2025-07-03T16:42:43.969384Z",
     "shell.execute_reply": "2025-07-03T16:42:43.968168Z",
     "shell.execute_reply.started": "2025-07-03T16:42:43.959217Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sequence Creation\n",
    "def create_rnn_sequences(processed_events, seq_length=SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Creates input (X) and output (y) sequences for RNN training.\n",
    "    Each event (pitch_duration) is mapped to a unique integer.\n",
    "    \n",
    "    Args:\n",
    "        processed_events (list): List of (pitch_duration) tuples.\n",
    "        seq_length (int): The length of the input sequence for the RNN.\n",
    "                          The RNN will learn to predict the (seq_length + 1)th event.\n",
    "                          \n",
    "    Returns:\n",
    "        tuple: (X, y, event_to_int, int_to_event, vocab_size)\n",
    "            - X (np.array): Input sequences for the RNN.\n",
    "            - y (np.array): Target output (next event) for the RNN.\n",
    "            - event_to_int (dict): Mapping from (pitch_duration) tuple to integer ID.\n",
    "            - int_to_event (dict): Mapping from integer ID to (pitch_duration) tuple.\n",
    "            - vocab_size (int): Total number of unique events.\n",
    "    \"\"\"\n",
    "    if not processed_events:\n",
    "        print(\"No processed events to create sequences from.\")\n",
    "        return None, None, None, None, 0\n",
    "\n",
    "    # 1. create a unified vocabulary and map to integers\n",
    "    # each unique (pitch_duration) tuple will be a single event for the RNN\n",
    "    unique_events = sorted(list(set(processed_events))) # sort for consistent mapping\n",
    "\n",
    "    event_to_int = {event: i for i, event in enumerate(unique_events)}\n",
    "    int_to_event = {i: event for i, event in enumerate(unique_events)}\n",
    "\n",
    "    vocab_size = len(unique_events)\n",
    "    print(f\"\\nCreated a vocabulary of {vocab_size} unique_events.\")\n",
    "    print(f\"Example mmapping: {list(event_to_int.items())[:5]}...\")\n",
    "\n",
    "    # convert the processed_events list into a list of integer IDs\n",
    "    numerical_events = [event_to_int[event] for event in processed_events]\n",
    "\n",
    "    # 2. create input-output pairs (sliding window)\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(numerical_events) - seq_length):\n",
    "        input_sequence = numerical_events[i: i + seq_length]\n",
    "        output_event = numerical_events[i + seq_length]\n",
    "\n",
    "        X.append(input_sequence)\n",
    "        y.append(output_event)\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    X = np.array(X)\n",
    "    # The output 'y' should be one-hot encoded for categorical cross-entropy loss\n",
    "    # However, if using sparse_categorical_crossentropy, it can remain as integer IDs.\n",
    "    # Let's keep it as integer IDs for now, as it's simpler and works with sparse_categorical_crossentropy.\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Created {len(X)} input-output sequence pairs.\")\n",
    "    print(f\"Shape of X: {X.shape} (Number of sequences, Sequence Length)\")\n",
    "    print(f\"Shape of y: {y.shape} (Number of target events)\")\n",
    "    print(f\"First input sequence (X[0]): {X[0]}\")\n",
    "    print(f\"First target event (y[0]): {y[0]}\")\n",
    "\n",
    "    return X, y, event_to_int, int_to_event, vocab_size\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_notes, midi_obj = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "    \n",
    "#     if extracted_notes:\n",
    "#         processed_events, unique_pitches_set, unique_durations_set = process_notes_for_rnn(\n",
    "#             extracted_notes, quantization_steps_per_beat=4\n",
    "#         )\n",
    "        \n",
    "#         processed_events = [f\"{pitch}_{duration:.3f}\" for pitch, duration in processed_events]\n",
    "\n",
    "        \n",
    "#         if processed_events:\n",
    "#             X, y, event_to_int, int_to_event, vocab_size = create_rnn_sequences(\n",
    "#                 processed_events, seq_length=SEQUENCE_LENGTH\n",
    "#             )\n",
    "            \n",
    "#             # These variables (X, y, event_to_int, int_to_event, vocab_size)\n",
    "#             # are now ready to be used in Step 4 for building and training the RNN.\n",
    "            \n",
    "#             print(\"\\nStep 3: Sequence Creation Complete.\")\n",
    "#             print(\"Your data is now prepared into input-output sequences for the RNN.\")\n",
    "#         else:\n",
    "#             print(\"No processed events to create sequences from. Check previous steps.\")\n",
    "#     else:\n",
    "#         print(\"MIDI file could not be processed. Please check the path and file validity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa883d89-c7d3-4261-bec7-b06891373d4c",
    "_uuid": "04ca0083-b0b4-4337-8bb9-6b735f34eadb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Step 4: Model Building\n",
    "\n",
    "**Goal of this step:**\n",
    "\n",
    "1. Define a sequential Keras model.\n",
    "\n",
    "2. Add an Embedding layer to convert integer IDs into dense vectors.\n",
    "\n",
    "3. Add one or more LSTM layers to learn temporal patterns.\n",
    "\n",
    "4. Add a Dense output layer with softmax activation to predict the next event.\n",
    "\n",
    "5. Compile the model with an appropriate loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ea2e790-0afb-414e-bd47-fafbb4a6c94e",
    "_uuid": "edcd7f43-cd7b-4c9e-812c-303500bb9ba3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:42:48.003650Z",
     "iopub.status.busy": "2025-07-03T16:42:48.003316Z",
     "iopub.status.idle": "2025-07-03T16:42:48.012908Z",
     "shell.execute_reply": "2025-07-03T16:42:48.011668Z",
     "shell.execute_reply.started": "2025-07-03T16:42:48.003625Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "# Model Building\n",
    "def build_rnn_model(vocab_size, seq_length=SEQUENCE_LENGTH, embedding_dim=EMBEDDING_DIM, lstm_units=LSTM_UNITS):\n",
    "    \"\"\"\n",
    "    Builds a Sequential Keras RNN model for music generation.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): The total number of unique events in our vocabulary.\n",
    "        seq_length (int): The length of the input sequences (timesteps).\n",
    "        embedding_dim (int): The dimension of the embedding vector for each event.\n",
    "        lstm_units (int): The number of LSTM units (neurons) in the hidden layer.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # 1. Embedding Layer: Converts integer inputs into dense vectors\n",
    "        # input_dim: size of the vocabulary (max integer index + 1)\n",
    "        # output_dim: dimension of the dense embedding\n",
    "        # input_length: length of input sequences\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length),\n",
    "        \n",
    "        # 2. LSTM Layer: The core of the RNN, learns sequential patterns\n",
    "        # return_sequences=True: Important if stacking multiple LSTM layers,\n",
    "        #                       makes the layer return a sequence of outputs for each timestep.\n",
    "        #                       For a single LSTM layer before Dense, can be False.\n",
    "        LSTM(lstm_units, return_sequences=True),\n",
    "        Dropout(0.3), # regularization to prevent overfitting\n",
    "        LSTM(lstm_units),\n",
    "\n",
    "        # 3. Dense Output Layer: Predicts the probability of the next event\n",
    "        # units: Number of possible output classes (our vocabulary size)\n",
    "        # activation='softmax': Converts raw scores into a probability distribution over the vocabulary\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "        \n",
    "    ])\n",
    "    # Compile the model\n",
    "    # optimizer: Adam is a good general-purpose optimizer\n",
    "    # loss: sparse_categorical_crossentropy is used when target labels are integer encoded (0, 1, 2...)\n",
    "    # metrics: 'accuracy' to monitor classification accuracy during training\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"\\n___ RNN Model Architecture ___\")\n",
    "    model.summary()\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_notes, midi_obj = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "    \n",
    "#     if extracted_notes:\n",
    "#         processed_events, unique_pitches_set, unique_durations_set = process_notes_for_rnn(\n",
    "#             extracted_notes, quantization_steps_per_beat=4\n",
    "#         )\n",
    "\n",
    "#         processed_events = [f\"{pitch}_{duration:.3f}\" for pitch, duration in processed_events]\n",
    "\n",
    "        \n",
    "#         if processed_events:\n",
    "#             X, y, event_to_int, int_to_event, vocab_size = create_rnn_sequences(\n",
    "#                 processed_events, seq_length=SEQUENCE_LENGTH\n",
    "#             )\n",
    "            \n",
    "#             if X is not None and y is not None:\n",
    "#                 model = build_rnn_model(vocab_size, seq_length=SEQUENCE_LENGTH)\n",
    "                \n",
    "#                 # Now the `model` object is ready for training in Step 5.\n",
    "#                 print(\"\\nStep 4: Model Building Complete.\")\n",
    "#                 print(\"Your Keras RNN model has been defined and compiled.\")\n",
    "#             else:\n",
    "#                 print(\"Could not create sequences. Check previous steps.\")\n",
    "#         else:\n",
    "#             print(\"No processed events to create sequences from. Check previous steps.\")\n",
    "#     else:\n",
    "#         print(\"MIDI file could not be processed. Please check the path and file validity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ec68b03-a9bb-45c0-9bff-cd42d254507c",
    "_uuid": "1e89bd01-3637-488e-931e-bb5b7ad10e2b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Step 5: Model Training\n",
    "\n",
    "**Goal of this step:**\n",
    "\n",
    "1. Train the model object using model.fit().\n",
    "\n",
    "2. Set up callbacks to save the best model weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d298b555-f996-42d7-bc97-9cda312356a7",
    "_uuid": "e42eb39f-6049-4dbe-bb2f-14eff800d18b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:42:54.221457Z",
     "iopub.status.busy": "2025-07-03T16:42:54.220630Z",
     "iopub.status.idle": "2025-07-03T16:42:54.228467Z",
     "shell.execute_reply": "2025-07-03T16:42:54.227375Z",
     "shell.execute_reply.started": "2025-07-03T16:42:54.221429Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "MODEL_WEIGHTS_PATH = '/kaggle/working/music_model_weights.h5'\n",
    "\n",
    "def train_model(model, X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, model_weights_path=MODEL_WEIGHTS_PATH):\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath = model_weights_path,\n",
    "        monitor = 'loss',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    print(f\"\\n___ Starting Model Training ({epochs}) epochs) ___\")\n",
    "    history = model.fit(\n",
    "        X, \n",
    "        y,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        callbacks = [checkpoint_callback],\n",
    "        verbose = 1\n",
    "    )\n",
    "    print(\"___ Model Training Complete ___\")\n",
    "    return history\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     extracted_notes, midi_obj = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "    \n",
    "#     if extracted_notes:\n",
    "#         processed_events, unique_pitches_set, unique_durations_set = process_notes_for_rnn(\n",
    "#             extracted_notes, quantization_steps_per_beat=4\n",
    "#         )\n",
    "\n",
    "#         processed_events = [f\"{pitch}_{duration:.3f}\" for pitch, duration in processed_events]\n",
    "        \n",
    "#         if processed_events:\n",
    "#             X, y, event_to_int, int_to_event, vocab_size = create_rnn_sequences(\n",
    "#                 processed_events, seq_length=SEQUENCE_LENGTH\n",
    "#             )\n",
    "            \n",
    "#             if X is not None and y is not None and len(X) > 0: # Ensure sequences were created\n",
    "#                 model = build_rnn_model(vocab_size, seq_length=SEQUENCE_LENGTH)\n",
    "                \n",
    "#                 # Check if enough data for training\n",
    "#                 if len(X) < BATCH_SIZE:\n",
    "#                     print(f\"Warning: Not enough data for one batch ({len(X)} sequences vs {BATCH_SIZE} batch size). \"\n",
    "#                           \"Training might be unstable or fail. Consider reducing BATCH_SIZE or increasing dataset size.\")\n",
    "#                     # Adjust batch size if dataset is too small\n",
    "#                     if len(X) > 0:\n",
    "#                         BATCH_SIZE = len(X) # Use all data as one batch\n",
    "#                         print(f\"Adjusted BATCH_SIZE to {BATCH_SIZE}.\")\n",
    "#                     else:\n",
    "#                         print(\"Cannot train: No sequences created after preprocessing.\")\n",
    "#                         exit()\n",
    "\n",
    "    #             history = train_model(model, X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, model_weights_path=MODEL_WEIGHTS_PATH)\n",
    "                \n",
    "    #             print(\"\\nStep 5: Model Training Complete.\")\n",
    "    #             print(f\"Trained model weights saved to: {MODEL_WEIGHTS_PATH}\")\n",
    "    #             # You can access training history: history.history['loss'], history.history['accuracy']\n",
    "    #         else:\n",
    "    #             print(\"Could not create sufficient sequences for training. Check previous steps and dataset size.\")\n",
    "    #     else:\n",
    "    #         print(\"No processed events to create sequences from. Check previous steps.\")\n",
    "    # else:\n",
    "    #     print(\"MIDI file could not be processed. Please check the path and file validity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c263ced-8d69-4ef7-a0ea-20b3050d64c5",
    "_uuid": "70a4dc67-e2b4-48f0-b6e7-e0b7bb495128",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Step 6: Music Generation\n",
    "\n",
    "**Goal of this step:**\n",
    "\n",
    "1. Load the trained model weights.\n",
    "\n",
    "2. Choose a \"seed\" sequence to start the generation.\n",
    "\n",
    "3. Implement a generation loop:\n",
    "\n",
    "   - Predict the next event based on the current sequence.\n",
    "\n",
    "   - Sample an event from the model's probability distribution (to add creativity).\n",
    "\n",
    "   - Append the sampled event to the generated sequence.\n",
    "\n",
    "   - Repeat for a desired number of new events.\n",
    "\n",
    "4. Convert the generated sequence of integer IDs back to (pitch, duration) tuples.\n",
    "\n",
    "5. construct a pretty_midi.PrettyMIDI object from these tuples.\n",
    "\n",
    "6. Save the generated music as a new MIDI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e7c4e0b-efe7-4459-b7b3-f76906b2c0fc",
    "_uuid": "cfc23adb-02d0-4373-9711-faad1d370f94",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:42:58.992693Z",
     "iopub.status.busy": "2025-07-03T16:42:58.992371Z",
     "iopub.status.idle": "2025-07-03T16:42:58.998671Z",
     "shell.execute_reply": "2025-07-03T16:42:58.997614Z",
     "shell.execute_reply.started": "2025-07-03T16:42:58.992671Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model # Import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import random\n",
    "\n",
    "GENERATED_MIDI_PATH = '/kaggle/working/generated_melody.mid' # Output path for the generated MIDI file\n",
    "GENERATION_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b617cc33-58e8-41eb-a13f-6f50b8657c89",
    "_uuid": "a153a5a2-5eaf-4110-bdb4-39ce5704133e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:43:01.628948Z",
     "iopub.status.busy": "2025-07-03T16:43:01.628533Z",
     "iopub.status.idle": "2025-07-03T16:43:01.641326Z",
     "shell.execute_reply": "2025-07-03T16:43:01.640134Z",
     "shell.execute_reply.started": "2025-07-03T16:43:01.628922Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Music Generation\n",
    "def generate_music(model, seed_sequence, int_to_event, vocab_size, generation_length=GENERATION_LENGTH, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a new sequence of music events using the trained RNN model.\n",
    "    \"\"\"\n",
    "    generated_events_int = list(seed_sequence)  # Start with the initial seed\n",
    "    current_sequence = np.array(seed_sequence).reshape(1, -1)  \n",
    "\n",
    "    print(f\"\\n__ Generating {generation_length} new events ___\")\n",
    "\n",
    "    for i in range(generation_length):\n",
    "        # Predict the probabilities for the next event\n",
    "        prediction_probs = model.predict(current_sequence, verbose=0)[0]\n",
    "\n",
    "        # Apply temperature (for controlling randomness)\n",
    "        prediction_probs = np.exp(np.log(prediction_probs + 1e-10) / temperature)\n",
    "        prediction_probs /= np.sum(prediction_probs)\n",
    "\n",
    "        # Sample the next event ID\n",
    "        next_event_int = np.random.choice(vocab_size, p=prediction_probs)\n",
    "\n",
    "        # Append the new event to the sequence\n",
    "        generated_events_int.append(next_event_int)\n",
    "\n",
    "        # Update current_sequence: slide the window and append next_event_int\n",
    "        current_sequence = np.append(current_sequence[:, 1:], [[next_event_int]], axis=1)  \n",
    "\n",
    "    # Convert integer IDs back to (pitch_duration) strings\n",
    "    \n",
    "    generated_events_tuples = [int_to_event[event_id] for event_id in generated_events_int]  \n",
    "\n",
    "    print(\"___ Music Generation Complete ___\")\n",
    "    return generated_events_tuples\n",
    "\n",
    "\n",
    "def convert_events_to_midi(events_list, output_midi_path=GENERATED_MIDI_PATH, tempo=120):\n",
    "    \"\"\"\n",
    "    Converts a list of (pitch, duration) tuples into a MIDI file.\n",
    "    \n",
    "    Args:\n",
    "    events_list (list): List of (pitch, duration) tuples.\n",
    "    output_midi_path (str): Path to save the generated MIDI file.\n",
    "    tempo (int): Tempo in beats per minute (BPM) for the generated MIDI.\n",
    "    \"\"\"\n",
    "    if not events_list:\n",
    "        print(\"No events to convert to MIDI\")\n",
    "        return\n",
    "\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    # for a simple melody, lets use a piano instrument(program 0)\n",
    "    piano_program = pretty_midi.instrument_name_to_program(\"Acoustic Grand Piano\")\n",
    "    piano_instrument = pretty_midi.Instrument(program=piano_program)\n",
    "\n",
    "    current_time = 0.0\n",
    "    for event in events_list:\n",
    "        pitch, duration = event\n",
    "\n",
    "        # if we had a 'Rest' token, \n",
    "        if isinstance(pitch, str) and pitch == 'Rest':\n",
    "            current_time += duration\n",
    "        else:\n",
    "            # ensure pitch is within valid MIDI range (0-127)\n",
    "            pitch = int(max(0, min(127, pitch)))\n",
    "            # ensure duration is positive\n",
    "            note_duration_seconds = max(0.01, duration)\n",
    "\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=100,\n",
    "                pitch=pitch,\n",
    "                start=current_time,\n",
    "                end=current_time + note_duration_seconds\n",
    "            )\n",
    "            piano_instrument.notes.append(note)\n",
    "            current_time += note_duration_seconds\n",
    "    midi.instruments.append(piano_instrument)\n",
    "\n",
    "    try:\n",
    "        midi.write(output_midi_path)\n",
    "        print(f\"Generated MIDI saved to: {output_midi_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving MIDI file: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # --- Music Generation Execution ---\n",
    "#     # Pick a random seed sequence from your training data\n",
    "#     start_index = np.random.randint(0, len(X) - 1)\n",
    "#     seed_sequence_int = X[start_index] # Get a sequence from your training inputs\n",
    "    \n",
    "#     generated_music_events = generate_music(\n",
    "#         model, \n",
    "#         seed_sequence_int, \n",
    "#         int_to_event, \n",
    "#         vocab_size, \n",
    "#         generation_length=GENERATION_LENGTH,\n",
    "#         temperature=0.8 # Experiment with temperature!\n",
    "#     )\n",
    "    \n",
    "#     # Convert generated event strings back to (pitch, duration) tuples\n",
    "#     generated_music_events = [\n",
    "#         (int(event.split('_')[0]) if event.split('_')[0] != 'Rest' else 'Rest', \n",
    "#          float(event.split('_')[1])) \n",
    "#         for event in generated_music_events\n",
    "#     ]\n",
    "\n",
    "#     # Convert to MIDI and save\n",
    "#     convert_events_to_midi(generated_music_events, output_midi_path=GENERATED_MIDI_PATH)\n",
    "    \n",
    "#     print(\"\\nStep 6: Music Generation Complete!\")\n",
    "#     print(\"You can now open 'generated_melody.mid' with a MIDI player or convert it to WAV/MP3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74a95db2-7181-4168-a7af-be912356a02f",
    "_uuid": "dbbf705d-5d18-4e6c-9ae6-25fa72496e2b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-03T16:43:09.869719Z",
     "iopub.status.busy": "2025-07-03T16:43:09.869269Z",
     "iopub.status.idle": "2025-07-03T16:56:14.569471Z",
     "shell.execute_reply": "2025-07-03T16:56:14.568774Z",
     "shell.execute_reply.started": "2025-07-03T16:43:09.869688Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extracted_notes, midi_obj = load_and_inspect_midi(MIDI_FILE_PATH)\n",
    "    \n",
    "    if extracted_notes:\n",
    "        processed_events, unique_pitches_set, unique_durations_set = process_notes_for_rnn(\n",
    "            extracted_notes, quantization_steps_per_beat=4\n",
    "        )\n",
    "\n",
    "        processed_events = [f\"{pitch}_{duration:.3f}\" for pitch, duration in processed_events]\n",
    "        \n",
    "        if processed_events:\n",
    "            X, y, event_to_int, int_to_event, vocab_size = create_rnn_sequences(\n",
    "                processed_events, seq_length=SEQUENCE_LENGTH\n",
    "            )\n",
    "            \n",
    "            if X is not None and y is not None and len(X) > 0: # Ensure sequences were created\n",
    "                # Build model first (even if loading weights, need architecture)\n",
    "                model = build_rnn_model(vocab_size, seq_length=SEQUENCE_LENGTH)\n",
    "                \n",
    "                # Check if weights file exists. If not, train the model.\n",
    "                if os.path.exists(MODEL_WEIGHTS_PATH):\n",
    "                    print(f\"\\nLoading trained model weights from: {MODEL_WEIGHTS_PATH}\")\n",
    "                    model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "                    print(\"Model weights loaded.\")\n",
    "                else:\n",
    "                    print(f\"\\nModel weights not found at {MODEL_WEIGHTS_PATH}. Training the model...\")\n",
    "                    # Adjust batch size if dataset is too small\n",
    "                    if len(X) < BATCH_SIZE:\n",
    "                        if len(X) > 0:\n",
    "                            BATCH_SIZE_ADJUSTED = len(X) \n",
    "                            print(f\"Adjusted BATCH_SIZE to {BATCH_SIZE_ADJUSTED}.\")\n",
    "                        else:\n",
    "                            print(\"Cannot train: No sequences created after preprocessing.\")\n",
    "                            exit()\n",
    "                    else:\n",
    "                        BATCH_SIZE_ADJUSTED = BATCH_SIZE # Use default batch size\n",
    "\n",
    "                    train_model(model, X, y, epochs=EPOCHS, batch_size=BATCH_SIZE_ADJUSTED, model_weights_path=MODEL_WEIGHTS_PATH)\n",
    "                \n",
    "                # --- Music Generation Execution ---\n",
    "                # Pick a random seed sequence from your training data\n",
    "                start_index = np.random.randint(0, len(X) - 1)\n",
    "                seed_sequence_int = X[start_index] # Get a sequence from your training inputs\n",
    "\n",
    "                generated_music_events = generate_music(\n",
    "                    model, \n",
    "                    seed_sequence_int, \n",
    "                    int_to_event, \n",
    "                    vocab_size, \n",
    "                    generation_length=GENERATION_LENGTH,\n",
    "                    temperature=0.8 # Experiment with temperature!\n",
    "                )\n",
    "\n",
    "                # Convert generated event strings back to (pitch, duration) tuples\n",
    "                generated_music_events = [\n",
    "                    (int(event.split('_')[0]) if event.split('_')[0] != 'Rest' else 'Rest', \n",
    "                     float(event.split('_')[1])) \n",
    "                    for event in generated_music_events\n",
    "                ]\n",
    "                # Convert to MIDI and save\n",
    "                convert_events_to_midi(generated_music_events, output_midi_path=GENERATED_MIDI_PATH)\n",
    "\n",
    "                print(\"\\nStep 6: Music Generation Complete!\")\n",
    "                print(\"You can now open 'generated_melody.mid' with a MIDI player or convert it to WAV/MP3.\")\n",
    "\n",
    "            else:\n",
    "                print(\"Could not create sufficient sequences for training/generation. Check previous steps and dataset size.\")\n",
    "        else:\n",
    "            print(\"No processed events to create sequences from. Check previous steps.\")\n",
    "    else:\n",
    "        print(\"MIDI file could not be processed. Please check the path and file validity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c87fa0c1-ca49-4571-99e5-765ea826fcc3",
    "_uuid": "6ad7d67c-6b66-43c3-836b-ee9fa454c13a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1423230,
     "sourceId": 2356848,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
